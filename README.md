# Chat with PDFs using Local LLM 
Chat with PDFs using local Ollama Models using Lanchain

### Installing Ollama

Download Ollama from [Ollama's website](https://ollama.com/)

Pull the required models
        ollama pull mistral
        ollama pull nomic-embed-text

To view the list of models available
        ollama list

Add Ollama Models path to the Environment Variables
