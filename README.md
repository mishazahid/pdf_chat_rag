# Chat with PDFs using Local LLM 
Chat with PDFs using local Ollama Models and Langchain to keep documents secure and on-premises 

### Installing Ollama

* Download Ollama from [Ollama's website](https://ollama.com/)

* Pull the required models
```
        ollama pull mistral
        ollama pull nomic-embed-text
```

* To view the list of models available <br/>
```
        ollama list
```

* Add Ollama Models path to the Environment Variables
